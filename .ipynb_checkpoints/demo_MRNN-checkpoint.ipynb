{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim  \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from gumbel import *\n",
    "from ops import *\n",
    "import reader\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    init_scale = 0.1\n",
    "    max_epoch = 3\n",
    "    max_max_epoch = 50\n",
    "    batch_size = 20\n",
    "    display_step = 100\n",
    "    lr = 30.0\n",
    "    lr_decay = 0.5\n",
    "    keep_prob = 0.5\n",
    "    max_grad_norm = 0.25\n",
    "    vocab_size = 10000\n",
    "    tau0=5.0 # initial temperature\n",
    "    ANNEAL_RATE=0.1\n",
    "    MIN_TEMP=0.1\n",
    "#     alpha=1\n",
    "#     beta = 0.25\n",
    "    \n",
    "    # Network Parameters\n",
    "    input_size = 300\n",
    "    hidden_size = 900\n",
    "    num_steps = 35 # timesteps\n",
    "    num_layers = 1\n",
    "    K= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBInput(object):\n",
    "    \"\"\"The input data.\"\"\"\n",
    "    def __init__(self, config, data, name=None):\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data) // self.batch_size) - 1) // self.num_steps\n",
    "        self.input_data, self.targets = reader.ptb_producer(data, self.batch_size, self.num_steps, name=name)\n",
    "\n",
    "def PrintPTBSentence(data, dict):\n",
    "    n, timestep = data.shape\n",
    "    s = []\n",
    "    for i in range(n):\n",
    "        temp = []\n",
    "        for j in range(timestep):\n",
    "            temp.append(dict[data[i][j]])\n",
    "        s.append(temp)\n",
    "    s = np.array(s)\n",
    "    return s\n",
    "\n",
    "def Dropoutcell(config, is_training):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(config.hidden_size, forget_bias=0.0,state_is_tuple=True)\n",
    "    if is_training:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell,\n",
    "                                             output_keep_prob=config.keep_prob,\n",
    "                                             variational_recurrent=False,\n",
    "                                             dtype=tf.float32)\n",
    "    return cell\n",
    "\n",
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    fetches = {\n",
    "        \"NLL\": model.NLL,\n",
    "        \"final_state\": model.final_state,\n",
    "        \"KL\": model.KL\n",
    "    }\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "\n",
    "    for step in range(model.data.epoch_size):\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.initial_state[0]] = state.c\n",
    "        feed_dict[model.initial_state[1]] = state.h\n",
    "        \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals[\"NLL\"]\n",
    "        state = vals[\"final_state\"]\n",
    "        KL = vals[\"KL\"]\n",
    "\n",
    "        costs += cost\n",
    "        iters += model.data.num_steps\n",
    "\n",
    "        if verbose and step % (model.data.epoch_size // 10) == 10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps NLL: %.3f KL: %f\" %\n",
    "                  (step * 1.0 / model.data.epoch_size, np.exp(costs / iters),\\\n",
    "                   iters * model.data.batch_size /(time.time() - start_time),\\\n",
    "                   cost,KL))\n",
    "\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, config, data):\n",
    "        training = tf.placeholder(tf.bool)\n",
    "        self.data = data\n",
    "        self.batch_size = data.batch_size\n",
    "        self.num_steps = data.num_steps\n",
    "        self.tau = tf.Variable(0.0, trainable=False)\n",
    "        self.lr  = tf.Variable(0.0, trainable=False)\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.embedding = tf.get_variable(\"embedding\", [config.vocab_size, config.input_size], dtype=tf.float32)\n",
    "            self.inputs = tf.nn.embedding_lookup(self.embedding, self.data.input_data)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            self.inputs = tf.nn.dropout(self.inputs, config.keep_prob)\n",
    "        with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE ):\n",
    "            \n",
    "            self.stcells = [Dropoutcell(config, is_training) for _ in range(config.K)]\n",
    "            self.initial_state = self.stcells[0].zero_state(self.batch_size, dtype=tf.float32)\n",
    "            \n",
    "            self.state = self.initial_state\n",
    "            self.outputs = []\n",
    "            self.ALL_z = []\n",
    "            self.ALL_qz = []\n",
    "        with tf.variable_scope(\"RNN\", reuse=tf.AUTO_REUSE):\n",
    "            for time_step in range(self.num_steps):\n",
    "                with tf.variable_scope('logit_enc'):\n",
    "                    logit_z = linear(self.inputs[:, time_step, :], config.K, name='L1_x_zin') + linear(self.state[1], config.K, name='L1_h_zin')\n",
    "                q_z = tf.nn.softmax(logit_z)\n",
    "                if is_training is True:\n",
    "                    z = gumbel_softmax(logit_z,self.tau,hard=False)\n",
    "                else:\n",
    "                    z = softmax_sample(logit_z)\n",
    "                self.ALL_z.append(z)\n",
    "                self.ALL_qz.append(q_z)\n",
    "                \n",
    "                h = []\n",
    "                c = []\n",
    "                for i in range(config.K):\n",
    "                    temp_name = 'L1_LSTM_'+str(i)\n",
    "                    with tf.variable_scope(temp_name):\n",
    "                        temp_h, temp_hc = self.stcells[i](self.inputs[:, time_step, :], self.state)\n",
    "                    h.append(temp_h)\n",
    "                    temp_c, _ = temp_hc\n",
    "                    c.append(temp_c)\n",
    "                h = tf.reshape(h, [config.K,self.batch_size,config.hidden_size])\n",
    "                c = tf.reshape(c, [config.K, self.batch_size, config.hidden_size])\n",
    "                new_h = tf.einsum('knd,nk->nd',h,z)\n",
    "                new_c = tf.einsum('knd,nk->nd',c,z)\n",
    "                self.cell_output = new_h\n",
    "                self.state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
    "                self.outputs.append(self.cell_output)\n",
    "        self.arrange_outputs = tf.reshape(tf.concat(self.outputs, 1), [-1, config.hidden_size])\n",
    "        self.logits = tf.reshape(linear(self.arrange_outputs, config.vocab_size), [self.batch_size, self.num_steps, config.vocab_size])\n",
    "        \n",
    "        cross_entropy = tf.contrib.seq2seq.sequence_loss(\n",
    "            self.logits,\n",
    "            self.data.targets,\n",
    "            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "        self.NLL = tf.reduce_sum(cross_entropy)\n",
    "        ALL_q_z_tmp = tf.transpose(self.ALL_qz,[1,0,2])\n",
    "        KL_tmp = ALL_q_z_tmp*(tf.log(ALL_q_z_tmp+1e-20)-tf.log(1.0/config.K))\n",
    "        self.KL = tf.reduce_mean(tf.reduce_sum(KL_tmp, [1, 2]))\n",
    "        self.cost = self.NLL\n",
    "        self.final_state = self.state\n",
    "        \n",
    "        if not is_training:\n",
    "            return\n",
    "    \n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),config.max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "    \n",
    "        self.new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self.lr_update = tf.assign(self.lr, self.new_lr)\n",
    "        self.new_tau = tf.placeholder(tf.float32, shape=[], name=\"new_tau\")\n",
    "        self.tau_update = tf.assign(self.tau, self.new_tau)\n",
    "        \n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self.lr_update, feed_dict={self.new_lr: lr_value})\n",
    "    def assign_tau(self, session, tau_value):\n",
    "        session.run(self.tau_update, feed_dict={self.new_tau: tau_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "raw_data = reader.ptb_raw_data('./data/')\n",
    "train_data, valid_data, test_data, w2id, id2w = raw_data\n",
    "config = Config()\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1\n",
    "\n",
    "URL = \"./checkpoint/MRNN_D=\"+str(config.hidden_size)+\"_K=\"+str(config.K)\n",
    "# os.makedirs(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initializer = tf.contrib.layers.xavier_initializer()\n",
    "# initializer = tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n",
    "initializer = tf.orthogonal_initializer()\n",
    "\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "    with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(is_training=True, config=config, data=train_input)\n",
    "\n",
    "with tf.name_scope(\"Valid\"):\n",
    "    valid_input = PTBInput(config=config, data=valid_data, name=\"ValidInput\")\n",
    "    with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(is_training=False, config=config, data=valid_input)\n",
    "\n",
    "with tf.name_scope(\"Test\"):\n",
    "    test_input = PTBInput(config=eval_config, data=test_data, name=\"TestInput\")\n",
    "    with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "        mtest = PTBModel(is_training=False, config=config, data=test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Launch the graph\n",
    "# sv = tf.train.Supervisor()\n",
    "# with sv.managed_session() as sess:\n",
    "#     state = sess.run(m.initial_state)\n",
    "#     feed_dict = {}\n",
    "#     for i, (c, h) in enumerate(m.initial_state):\n",
    "#         feed_dict[c] = state[i].c\n",
    "#         feed_dict[h] = state[i].h\n",
    "\n",
    "#     print(m.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "sv = tf.train.Supervisor()\n",
    "with sv.managed_session() as sess:\n",
    "    \n",
    "    ppl_train = []\n",
    "    ppl_valid = [np.inf]\n",
    "    m.assign_lr(sess, config.lr)\n",
    "    start_time = time.time()\n",
    "    for i in range(config.max_max_epoch):\n",
    "        \n",
    "        np_temp=np.maximum(config.tau0*np.exp(-config.ANNEAL_RATE*i),config.MIN_TEMP)\n",
    "        m.assign_tau(sess, np_temp)\n",
    "        temp_lr, temp_tau = sess.run([m.lr, m.tau])\n",
    "        if temp_lr < 0.0001:\n",
    "            break\n",
    "        \n",
    "        print(\"Epoch: %d Learning rate: %f, Temperature: %f\" % (i + 1, temp_lr, temp_tau))\n",
    "        train_perplexity = run_epoch(sess, m, eval_op=m.train_op,verbose=True)\n",
    "        ppl_train.append(train_perplexity)\n",
    "        print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        valid_perplexity = run_epoch(sess, mvalid)\n",
    "        print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "        ppl_valid.append(valid_perplexity)\n",
    "\n",
    "        if (i+1) > config.max_epoch:\n",
    "            if ppl_valid[-2] - ppl_valid[-1] < 0:\n",
    "                m.assign_lr(sess, temp_lr * config.lr_decay)\n",
    "                \n",
    "#         if (i+1) > config.max_epoch:\n",
    "#             m.assign_lr(sess, temp_lr * config.lr_decay)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    test_perplexity = run_epoch(sess, mtest)\n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "    print(\"Training time: %f, Testing time: %f\" % (end_time-start_time,time.time()-end_time))\n",
    "    \n",
    "#     sv.saver.save(sess, URL+\"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_para = 0\n",
    "for var in tf.trainable_variables():\n",
    "    shape = var.get_shape()\n",
    "    tmp = 1\n",
    "    for dim in shape:\n",
    "        tmp *= dim.value\n",
    "    total_para += tmp\n",
    "print('Total parameters: ', total_para)\n",
    "print(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy for 128 mnist test images\n",
    "plt.figure()\n",
    "plt.plot(range(1,len(ppl_train)+1),ppl_train,'b')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(range(1,len(ppl_valid)+1),ppl_valid,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"temp1.csv\", ppl_train, delimiter=\",\")\n",
    "np.savetxt(\"temp2.csv\", ppl_valid, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sv = tf.train.Supervisor()\n",
    "with sv.managed_session() as sess:\n",
    "    sv.saver.restore(sess, URL+\"/model.ckpt\")\n",
    "    test_perplexity = run_epoch(sess, mtest)\n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=2)\n",
    "\n",
    "model = mtest\n",
    "sv = tf.train.Supervisor()\n",
    "z = []\n",
    "qz = []\n",
    "h = []\n",
    "x = []\n",
    "start_time = time.time()\n",
    "with sv.managed_session() as sess:\n",
    "    sv.saver.restore(sess, URL+\"/model.ckpt\")\n",
    "\n",
    "    state = sess.run(model.initial_state)\n",
    "    fetches = {\n",
    "        \"z\": model.ALL_z,\n",
    "        \"qz\": model.ALL_qz,\n",
    "        \"h\": model.state,\n",
    "        \"x\": model.data.input_data,\n",
    "        \"y\": model.data.targets,\n",
    "        \"logits\":model.logits,\n",
    "        \"final_state\": model.final_state,\n",
    "    }\n",
    "    print(model.data.epoch_size)\n",
    "    for i in range(model.data.epoch_size):\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.initial_state[0]] = state.c\n",
    "        feed_dict[model.initial_state[1]] = state.h\n",
    "        vals = sess.run(fetches, feed_dict)\n",
    "        state = vals[\"final_state\"]\n",
    "        \n",
    "        \n",
    "        h.append(vals['h'])\n",
    "        qz.append(vals['qz'][0][0])\n",
    "        z.append(vals['z'][0][0])\n",
    "        x.append(vals['x'][0][0])\n",
    "        \n",
    "        \n",
    "#         print(id2w[vals['x'][0][0]])\n",
    "#         print(id2w[vals['y'][0][0]])\n",
    "#         print(id2w[np.argmax(vals['logits'])])\n",
    "#         print('-----------------')\n",
    "    end_time = time.time()\n",
    "    print(\"time: %f\" % (end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h = np.reshape(h,[model.data.epoch_size,2,-1])\n",
    "z = np.array(z)\n",
    "qz = np.array(qz)\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "start_time = time.time()\n",
    "N = 1000\n",
    "h_tsne = TSNE(n_components=2).fit_transform(h[:N,1,:])\n",
    "print(\"time: %f\" % (time.time()-start_time))\n",
    "np.save('./tsne_MRNN_K=4',h_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "color = ['r','g','b','y']\n",
    "for i in range(N):\n",
    "    plt.scatter(h_tsne[i,0], h_tsne[i,1], c=color[np.argmax(z,1)[i]], marker='.')\n",
    "#     plt.scatter(h_tsne[i,0], h_tsne[i,1], h_tsne[i,2], c=color[np.argmax(z,1)[i]], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "start_time = time.time()\n",
    "h_tsne = TSNE(n_components=2).fit_transform(np.reshape(h,[model.data.epoch_size,-1])[:N,:])\n",
    "print(\"time: %f\" % (time.time()-start_time))\n",
    "np.save('./tsne_MRNN_K=4',h_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "color = ['r','g','b','y']\n",
    "for i in range(N):\n",
    "    plt.scatter(h_tsne[i,0], h_tsne[i,1], c=color[np.argmax(z,1)[i]], marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,15])\n",
    "plt.imshow(z[:100,:].T)\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=[15,15])\n",
    "plt.imshow(qz[:100,:].T)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-virtual",
   "language": "python",
   "name": "python3-virtual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
